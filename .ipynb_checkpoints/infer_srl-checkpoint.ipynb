{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import read_data\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from optparse import OptionParser\n",
    "import torch.autograd as autograd\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import sys\n",
    "import pprint\n",
    "import numpy as np\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "torch.manual_seed(1)\n",
    "\n",
    "from seqeval.metrics import accuracy_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "# from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "import eval_srl\n",
    "from src import etri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASKING = False\n",
    "ARGINFO = True\n",
    "DPINFO = True\n",
    "\n",
    "model_dir = './result/model-morp-sum'\n",
    "model_path = model_dir+'/model.pt'\n",
    "config = model_dir+'/config.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:391: UserWarning: Couldn't retrieve source code for container of type LSTMTagger. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config,'r') as f:\n",
    "    configuration = json.load(f)\n",
    "\n",
    "TOKDIM = configuration['token_dim']\n",
    "if DPINFO:\n",
    "    DPDIM = configuration['dp_dim']\n",
    "else:\n",
    "    DPDIM = 0\n",
    "ARGDIM = configuration['arg_dim']\n",
    "LSTMINPDIM = configuration['lstm_input_dim']\n",
    "if ARGINFO:\n",
    "    FEATDIM = configuration['feat_dim']\n",
    "else:\n",
    "    FEATDIM = 1\n",
    "HIDDENDIM = configuration['hidden_dim']\n",
    "LSTMDEPTH = configuration['lstm_depth']\n",
    "DROPOUT_RATE = configuration['dropout_rate']\n",
    "learning_rate = configuration['learning_rate']\n",
    "NUM_EPOCHS = configuration['num_epochs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        \n",
    "        if DPINFO:\n",
    "            self.dp_embeddings = nn.Embedding(DP_VOCAB_SIZE, DPDIM)\n",
    "        \n",
    "        self.lstm_tok = nn.LSTM(LSTMINPDIM+TOKDIM+DPDIM+FEATDIM, HIDDENDIM//2, bidirectional=True, num_layers=LSTMDEPTH, dropout=DROPOUT_RATE)\n",
    "        self.hidden_lstm_tok = self.init_hidden_lstm_tok()\n",
    "        \n",
    "        # Linear\n",
    "        self.hidden2tag = nn.Linear(HIDDENDIM, tagset_size)\n",
    "    \n",
    "    def init_hidden_lstm_tok(self):\n",
    "        return (torch.zeros(4, 1, HIDDENDIM//2).cuda(),\n",
    "            torch.zeros(4, 1, HIDDENDIM//2).cuda())\n",
    "    \n",
    "    def forward(self, input_sent, pred_idx, dp_in, feat_vector, mask):\n",
    "        \n",
    "        if DPINFO:\n",
    "            dp_embs = self.dp_embeddings(dp_in)\n",
    "        \n",
    "#         LSTM layer 1 (subunit to token)\n",
    "        tok_vectors = []\n",
    "    \n",
    "        pred_vec = torch.zeros(100).cuda()\n",
    "        for morp in input_sent[pred_idx]:\n",
    "            pred_vec += get_word2vec(morp)\n",
    "    \n",
    "        for morps in input_sent:\n",
    "            we = torch.zeros(100).cuda()\n",
    "            for morp in morps:\n",
    "                we += get_word2vec(morp)\n",
    "            we = torch.cat( (we, pred_vec) )\n",
    "            tok_vectors.append(we)\n",
    "\n",
    "        tok_vec = torch.stack(tok_vectors)\n",
    "        tok_vec = tok_vec.view(len(tok_vec), -1)\n",
    "        \n",
    "#         LSTM layer\n",
    "        if DPINFO:\n",
    "            input_embs = torch.cat( (tok_vec, dp_embs, feat_vector), 1)\n",
    "        else:\n",
    "            input_embs = torch.cat( (tok_vec, feat_vector), 1)\n",
    "        input_embs_2 = input_embs.view(len(input_embs), 1, -1)\n",
    "\n",
    "        lstm_out_tok, self.hidden_lstm_tok = self.lstm_tok(\n",
    "            input_embs_2, self.hidden_lstm_tok)\n",
    "        \n",
    "#         lstm_out_tok = F.relu(lstm_out_tok)\n",
    "        \n",
    "        # Linear\n",
    "        tag_space = self.hidden2tag(lstm_out_tok.view(len(input_embs_2),-1))  \n",
    "   \n",
    "        return tag_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP_VOCAB_SIZE: 32\n",
      "ARG_VOCAB_SIZE: 18\n",
      "MORP_VOCAB_SIZE: 63376\n"
     ]
    }
   ],
   "source": [
    "data = read_data.load_trn_data()\n",
    "trn_conll = read_data.load_trn_nlp()\n",
    "\n",
    "def prepare_idx():\n",
    "    dp_to_ix, arg_to_ix, morp_to_ix = {},{},{}\n",
    "    dp_to_ix['null'] = 0\n",
    "    morp_to_ix['null'] = 0\n",
    "    \n",
    "    for sent in trn_conll:\n",
    "        for token in sent:\n",
    "            dp = token[11]\n",
    "            if dp not in dp_to_ix:\n",
    "                dp_to_ix[dp] = len(dp_to_ix)\n",
    "                \n",
    "            morphs = token[2].split('+')\n",
    "            for morp in morphs:\n",
    "                if morp not in morp_to_ix:\n",
    "                    morp_to_ix[morp] = len(morp_to_ix)\n",
    "    args = ['ARG0', 'ARG1', 'ARG2', 'ARG3', 'ARGM-CAU', 'ARGM-CND', 'ARGM-DIR', 'ARM-DIS', 'ARGM-INS', 'ARGM-LOC', 'ARCM-MNR', 'ARCM-NEG', 'ARCM-PRD', 'ARCM-PRP', 'ARCM-TMP', 'ARCM-ADV', 'ARCM-EXT', '-']\n",
    "    for i in args:\n",
    "        if i not in arg_to_ix:\n",
    "            arg_to_ix[i] = len(arg_to_ix)\n",
    "    return dp_to_ix, arg_to_ix, morp_to_ix\n",
    "dp_to_ix, arg_to_ix, morp_to_ix = prepare_idx()\n",
    "DP_VOCAB_SIZE = len(dp_to_ix)\n",
    "ARG_VOCAB_SIZE = len(arg_to_ix)\n",
    "MORP_VOCAB_SIZE = len(morp_to_ix)\n",
    "print('DP_VOCAB_SIZE:',DP_VOCAB_SIZE)\n",
    "print('ARG_VOCAB_SIZE:',ARG_VOCAB_SIZE)\n",
    "print('MORP_VOCAB_SIZE:', MORP_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### loading word2vec model...\n",
      "... is done\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "print('### loading word2vec model...')\n",
    "wv_model = KeyedVectors.load_word2vec_format(\"./wordembedding/100_dim_3_window_5mincount_word2vec.model\")\n",
    "print('... is done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec(morp):\n",
    "    \n",
    "    vec = torch.rand(100)\n",
    "    emb = vec.cuda()\n",
    "    \n",
    "    try:\n",
    "        vec = wv_model[morp]\n",
    "        emb = torch.from_numpy(vec).cuda()\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except:\n",
    "        pass                \n",
    "\n",
    "    return emb\n",
    "\n",
    "def get_pred_idxs(conll):\n",
    "    result = []\n",
    "    preds = [0 for i in range(len(conll))]\n",
    "    for i in range(len(conll)):\n",
    "        tok = conll[i]\n",
    "        if tok[10] == 'VP' or tok[10] == 'VP_MOD' :\n",
    "            preds = [0 for item in range(len(conll))]\n",
    "            preds[i] = 1\n",
    "            result.append(preds)\n",
    "            \n",
    "    return result\n",
    "\n",
    "def get_arg_idxs(pred_idx, conll):\n",
    "    arg_idxs = [0 for i in range(len(conll))]\n",
    "    for i in range(len(conll)):\n",
    "        tok = conll[i]\n",
    "        if int(tok[8]) == pred_idx:\n",
    "#             arg_idxs[i] = 1\n",
    "            arg_pos = tok[-1]\n",
    "            if arg_pos[:2] == 'NP':\n",
    "                arg_idxs[i] = 1\n",
    "                \n",
    "    return arg_idxs\n",
    "\n",
    "def get_feature(pred_idx, conll):\n",
    "    feat_vec = []\n",
    "    arg_idxs = get_arg_idxs(pred_idx, conll)\n",
    "    for i in range(len(conll)):\n",
    "#         tok = conll[i]\n",
    "        feature = []\n",
    "        if pred_idx == i:\n",
    "            feature.append(1)\n",
    "        else:\n",
    "            feature.append(0)\n",
    "        if FEATDIM >= 2:\n",
    "            feature.append(arg_idxs[i])\n",
    "        if FEATDIM == 3:\n",
    "            if i >= pred_idx:\n",
    "                position = 0\n",
    "            else:\n",
    "                position = 1\n",
    "            feature.append(position)\n",
    "        feat_vec.append(feature)                \n",
    "        \n",
    "    return feat_vec\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    vocab = list(to_ix.keys())\n",
    "    idxs = []\n",
    "    for w in seq:\n",
    "        if w in vocab:\n",
    "            idxs.append(to_ix[w])\n",
    "        else:\n",
    "            idxs.append(0)  \n",
    "\n",
    "    return torch.tensor(idxs).cuda()\n",
    "\n",
    "def get_dps(conll):\n",
    "    dps = []\n",
    "    for tok in conll:\n",
    "        dp = tok[10]\n",
    "        dps.append(dp)\n",
    "    return dps\n",
    "\n",
    "def get_sentence_vec(tokens, conll):\n",
    "    result = []\n",
    "    for i in range(len(tokens)):\n",
    "        token = tokens[i]\n",
    "        morps = conll[i][2].split('+')\n",
    "#         morp_ix = prepare_sequence(morps, morp_to_ix)\n",
    "        result.append(morps)\n",
    "    return result\n",
    "\n",
    "def get_labels_by_tensor(t):\n",
    "    value, indices = t.max(1)\n",
    "    score = pow(1, value)\n",
    "    labels = []\n",
    "    for i in indices:\n",
    "        for label, idx in arg_to_ix.items():\n",
    "            if idx == i:\n",
    "                pred = label\n",
    "                labels.append(pred)\n",
    "                break\n",
    "    return labels, score            \n",
    "        \n",
    "    pred = None\n",
    "    for label, idx in arg_to_ix.items():\n",
    "        if idx == indices:\n",
    "            pred = label\n",
    "            break\n",
    "    return pred, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(data, my_model):\n",
    "    \n",
    "    result = []\n",
    "    for sent in data:\n",
    "        text = ' '.join(sent[1])\n",
    "        nlp = etri.getETRI_rest(text)\n",
    "        conll = etri.getETRI_CoNLL2009(nlp)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            tokens, args = sent[1], sent[2]            \n",
    "            args_in_all = prepare_sequence(args, arg_to_ix)\n",
    "            \n",
    "            dps = get_dps(conll)\n",
    "            dp_in = prepare_sequence(dps, dp_to_ix)\n",
    "\n",
    "            pred_idxs = get_pred_idxs(conll)\n",
    "\n",
    "            pred = ['-' for i in range(len(args))]\n",
    "\n",
    "            for i in range(len(pred_idxs)):\n",
    "                pred_seq = pred_idxs[i]\n",
    "                for j in range(len(pred_seq)):\n",
    "                    p = pred_seq[j]\n",
    "                    if p == 1:\n",
    "                        pred_idx = j\n",
    "                        \n",
    "                feature = get_feature(pred_idx, conll)\n",
    "                feat_vector = torch.tensor(feature).type(torch.cuda.FloatTensor)           \n",
    "                input_sent = get_sentence_vec(tokens, conll)\n",
    "                arg_idxs = get_arg_idxs(pred_idx, conll)            \n",
    "                args_in = torch.zeros(len(arg_idxs))\n",
    "\n",
    "                for idx in range(len(arg_idxs)):\n",
    "                    a = arg_idxs[idx]\n",
    "                    if a == 1:\n",
    "                        args_in[idx] = args_in_all[idx]\n",
    "                    else:\n",
    "                        args_in[idx] = 17\n",
    "\n",
    "                args_in = args_in.type(torch.cuda.LongTensor)\n",
    "                \n",
    "                mask = torch.tensor(arg_idxs).cuda()\n",
    "                mask = mask.float()\n",
    "\n",
    "                tag_scores = my_model(input_sent, pred_idx, dp_in, feat_vector, mask)\n",
    "                \n",
    "#                 print(tag_scores)\n",
    "                labels, score = get_labels_by_tensor(tag_scores)\n",
    "                \n",
    "                for idx in range(len(labels)):\n",
    "                    if arg_idxs[idx] == 1:\n",
    "                        label = labels[idx]\n",
    "                    else:\n",
    "                        label = '-'\n",
    "                        \n",
    "                    if label == '-':\n",
    "                        pass\n",
    "                    else:\n",
    "                        if pred[idx] == '-':\n",
    "                            pred[idx] = label\n",
    "                            \n",
    "            result.append(pred)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#input\n",
      "[[['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15'], ['인사동에', '들어서면', '다종다양의', '창호지,', '도자기', '등', '고미술품들이', '진열장에', '즐비하게', '널려져', '있는', '것을', '볼', '수', '있다.'], ['ARGM-LOC', '-', '-', '-', '-', '-', 'ARG1', 'ARG1', '-', '-', '-', 'ARG1', '-', '-', '-']], [['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], ['올림픽에', '출진하는', '선수라면', '어떤', '금전적', '인유가', '없더라도', '최악을', '다하겠다는', '정신력,', '욕망은', '차있다.'], ['ARG3', '-', 'ARG0', '-', '-', 'ARG1', '-', 'ARG1', '-', '-', 'ARG1', '-']], [['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], ['젖먹이들은', '비닐', '찢어', '우산살', '일체', '붙인', '졸악한', '것을', '연이라', '믿고', '있을', '터인데.'], ['ARG0', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-']], [['1', '2', '3', '4', '5'], ['때로는', '규범이', '큰', '무도화도', '있다.'], ['-', 'ARG1', '-', 'ARG1', '-']], [['1', '2', '3', '4', '5', '6'], ['삶이란', '끊임없는', '취택의', '불연속이라고', '나는', '요탁했다.'], ['-', '-', '-', '-', 'ARG0', '-']]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:49: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#output\n",
      "[['ARG3', '-', '-', '-', '-', '-', '-', 'ARG0', '-', '-', '-', 'ARG1', '-', '-', '-'], ['ARG3', '-', '-', '-', '-', 'ARG1', '-', 'ARG1', '-', '-', 'ARG1', '-'], ['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], ['-', 'ARG1', '-', 'ARG1', '-'], ['-', '-', '-', '-', 'ARG0', '-']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['ARG3',\n",
       "  '-',\n",
       "  '-',\n",
       "  '-',\n",
       "  '-',\n",
       "  '-',\n",
       "  '-',\n",
       "  'ARG0',\n",
       "  '-',\n",
       "  '-',\n",
       "  '-',\n",
       "  'ARG1',\n",
       "  '-',\n",
       "  '-',\n",
       "  '-'],\n",
       " ['ARG3', '-', '-', '-', '-', 'ARG1', '-', 'ARG1', '-', '-', 'ARG1', '-'],\n",
       " ['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'],\n",
       " ['-', 'ARG1', '-', 'ARG1', '-'],\n",
       " ['-', '-', '-', '-', 'ARG0', '-']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test():\n",
    "    data = read_data.load_trn_data()\n",
    "    dummy_data = []\n",
    "    for sent in data:\n",
    "        sent_list = []\n",
    "        \n",
    "        tok_idx = []\n",
    "        tok_str = []\n",
    "        tok_arg = []\n",
    "        for token in sent:\n",
    "            tok_idx.append(token[0])\n",
    "            tok_str.append(token[1])\n",
    "            tok_arg.append(token[2])\n",
    "            \n",
    "        sent_list.append(tok_idx)\n",
    "        sent_list.append(tok_str)\n",
    "        sent_list.append(tok_arg)\n",
    "        dummy_data.append(sent_list)\n",
    "    dummy_data = dummy_data[:5]\n",
    "    \n",
    "    print('#input')\n",
    "    print(dummy_data)\n",
    "    \n",
    "    result = infer(dummy_data, my_model)\n",
    "    print('#output')\n",
    "    print(result)\n",
    "    \n",
    "    return result\n",
    "        \n",
    "# result = test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
