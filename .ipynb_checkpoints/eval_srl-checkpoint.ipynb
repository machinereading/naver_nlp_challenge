{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*-encoding:utf-8-*-\n",
    "import argparse\n",
    "import sys\n",
    "import re\n",
    "import codecs\n",
    "from collections import defaultdict, namedtuple\n",
    "\n",
    "ANY_SPACE = '<SPACE>'\n",
    "\n",
    "class FormatError(Exception):\n",
    "    pass\n",
    "\n",
    "Metrics = namedtuple('Metrics', 'tp fp fn prec rec fscore')\n",
    "\n",
    "class EvalCounts(object):\n",
    "    def __init__(self):\n",
    "        self.correct = 0 # 'O'를 포함하여 세는 것. accuracy를 위해.\n",
    "        self.correct_tags = 0 # 'O'를 제외하고 세는 것.\n",
    "        self.found_correct = 0 #\n",
    "        self.found_guessed = 0 # number of identified arguments\n",
    "        self.num_words = 0 # token counter (ignores sentence breaks)\n",
    "\n",
    "        # counts by type\n",
    "        self.t_correct_tags = defaultdict(int)\n",
    "        self.t_found_correct = defaultdict(int)\n",
    "        self.t_found_guessed = defaultdict(int)\n",
    "\n",
    "def evaluate_from_list(prediction, gold):\n",
    "    counts = EvalCounts()\n",
    "    for p,g in zip(prediction, gold):\n",
    "        if p == g:\n",
    "            counts.correct += 1\n",
    "        if p == g and g not in ['-', 'O']:\n",
    "            counts.t_correct_tags[g] += 1\n",
    "            counts.correct_tags += 1\n",
    "\n",
    "        if g not in ['-', 'O']:\n",
    "            counts.found_correct += 1\n",
    "            counts.t_found_correct[g] += 1\n",
    "        if p not in ['-', 'O']:\n",
    "            counts.found_guessed += 1\n",
    "            counts.t_found_guessed[p] += 1\n",
    "        counts.num_words += 1\n",
    "    overall, by_type = metrics(counts)\n",
    "    if counts.num_words > 0:\n",
    "        return overall.fscore\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "def uniq(iterable):\n",
    "    seen = set()\n",
    "    return [i for i in iterable if not (i in seen or seen.add(i))]\n",
    "\n",
    "def calculate_metrics(correct, guessed, total):\n",
    "\n",
    "    tp, fp, fn = correct, guessed-correct, total-correct\n",
    "    p = 0 if tp + fp == 0 else 1.*tp / (tp + fp)\n",
    "    r = 0 if tp + fn == 0 else 1.*tp / (tp + fn)\n",
    "    f = 0 if p + r == 0 else 2 * p * r / (p + r)\n",
    "    return Metrics(tp, fp, fn, p, r, f)\n",
    "\n",
    "def metrics(counts):\n",
    "    c = counts\n",
    "    overall = calculate_metrics(c.correct_tags, c.found_guessed, c.found_correct)\n",
    "    by_type = {}\n",
    "    for t in uniq(list(c.t_found_correct) + list(c.t_found_guessed)):\n",
    "        by_type[t] = calculate_metrics(c.t_correct_tags[t], c.t_found_guessed[t], c.t_found_correct[t])\n",
    "    return overall, by_type\n",
    "\n",
    "def read_prediction(prediction_file):\n",
    "\n",
    "#     prediction file은 매 행에 문장 단위 argument label이 작성된 상태입니다.\n",
    "#     예) \"['ARG0', 'ARG1', '-', ..., '-']\\n['ARG0', '-', 'ARG3', ..., '-']\n",
    "#     이 함수는 평가 코퍼스 내 모든 결과를 모아 리턴합니다.\n",
    "\n",
    "\n",
    "    predictions = []\n",
    "    with open(prediction_file, encoding='utf-8') as fp:\n",
    "        for sentence in fp.read().strip().split('\\n'):\n",
    "            # sentence = \"['ARG0', 'ARG1', ..., '-']\"\n",
    "            predictions.extend(eval(sentence))\n",
    "    # predictions = ['ARG0', 'ARG1', 'ARG2', '-', 'ARG0', ..., ]\n",
    "    return predictions\n",
    "\n",
    "def read_ground_truth(ground_truth_file):\n",
    "    with open(ground_truth_file, encoding='utf-8') as fp:\n",
    "        ground_truths = [arg.strip() for arg in fp.read().strip().split('\\n') if arg.strip()]\n",
    "    return ground_truths\n",
    "\n",
    "def evaluation_metrics(prediction_file: str, ground_truth_file: str):\n",
    "#     read prediction and ground truth from file\n",
    "    prediction = read_prediction(prediction_file)\n",
    "    ground_truth = read_ground_truth(ground_truth_file)\n",
    "    return evaluate_from_list(prediction, ground_truth)\n",
    "\n",
    "def srl_eval(prediction, ground_truth):\n",
    "    return evaluate_from_list(prediction, ground_truth)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     args = argparse.ArgumentParser() # --prediction 은 inference 결과가 담긴 file 명으로 셋팅합니다. (nsml 내부적으로 셋팅함)\n",
    "#     args.add_argument('--prediction', type=str, default='pred.txt')\n",
    "#     config = args.parse_args()\n",
    "#     # dataset push 를 할때, leaderboard옵션으로 푸시하였으면, 자동으로 test/test_label 의 위치에 test_label 가 존재하므로, 해당위치로 설정합니다.\n",
    "#     test_label_path = '/data/SRL/test/test_label'\n",
    "#     # print the evaluation result\n",
    "#     # evaluation 은 int 또는 float 값으로만 출력합니다.\n",
    "#     try:\n",
    "#         eval_result = evaluation_metrics(config.prediction, test_label_path)\n",
    "#         print (eval_result)\n",
    "#     except:\n",
    "#         # 에러로 인한 0점 처리\n",
    "#         print(\"0\")\n",
    "# '''\n",
    "# if __name__ == '__main__':\n",
    "# \tprint (evaluation_metrics('pred.txt', 'gold.txt'))\n",
    "# '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
